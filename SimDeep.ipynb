{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1097,"sourceType":"datasetVersion","datasetId":572},{"sourceId":94928,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":79608,"modelId":104089}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install flwr torch scikit-learn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport zipfile\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\nfrom sklearn.model_selection import train_test_split\nfrom typing import Dict, List, Optional, Tuple\nimport flwr as fl   \nfrom flwr.server.strategy import FedAvg\nfrom flwr.server.client_manager import ClientManager\nfrom flwr.server.client_proxy import ClientProxy\nfrom flwr.common import NDArrays, Parameters, Scalar, FitRes, EvaluateRes, ndarrays_to_parameters, parameters_to_ndarrays\nfrom flwr.server import ServerConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\nimport umap\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/UjiIndoorLoc/TrainingData.csv', header=0)\ntest_data = pd.read_csv('/kaggle/input/UjiIndoorLoc/ValidationData.csv', header=0)\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\n\n# Extract features and device IDs\nfeatures = train_data.filter(regex='^WAP')  # Select only WAP columns\ndevice_ids = train_data['PHONEID']\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\n\n# Option 1: Dimensionality Reduction with PCA\npca = PCA(n_components=50)  # Reduce to 50 components (adjust as needed)\nfeatures_pca = pca.fit_transform(features_scaled)\n\n# Apply t-SNE on PCA-reduced features\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=0)\nfeatures_tsne = tsne.fit_transform(features_pca)\n\n# Create a DataFrame for plotting\ntsne_df = pd.DataFrame(data=features_tsne, columns=['TSNE1', 'TSNE2'])\ntsne_df['PHONEID'] = device_ids  # Add device IDs to the DataFrame\n\n# Plotting\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='PHONEID', palette='tab20', data=tsne_df, legend='full')\nplt.title('t-SNE Visualization of Non-IID Data by Device')\nplt.xlabel('t-SNE Axis 1')\nplt.ylabel('t-SNE Axis 2')\nplt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Device ID')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tsne_df = pd.DataFrame(data=features_tsne, columns=['TSNE1', 'TSNE2'])\ntsne_df['PHONEID'] = device_ids  # Add device IDs to the DataFrame\n\n# Plotting\nplt.figure(figsize=(12, 8))\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='PHONEID', palette='tab20', data=tsne_df, legend='full')\nplt.title('t-SNE Visualization of UJIIndoorLoc Dataset by Device')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Device ID')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ntrain_data = pd.read_csv('/kaggle/input/UjiIndoorLoc/TrainingData.csv', header=0)\n\n# Extract features and device IDs\nfeatures = train_data.filter(regex='^WAP')  # Select only WAP columns\ndevice_ids = train_data['PHONEID']  # Use 'PHONEID' for device identification\n\n# Standardize features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\n\n# Dimensionality Reduction with PCA before t-SNE (optional, but helps speed up t-SNE)\npca = PCA(n_components=50)  # Reduce to 50 components\nfeatures_pca = pca.fit_transform(features_scaled)\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200, random_state=0)\nfeatures_tsne = tsne.fit_transform(features_pca)\n\n# Create a DataFrame for plotting\ntsne_df = pd.DataFrame(data=features_tsne, columns=['TSNE1', 'TSNE2'])\ntsne_df['PHONEID'] = device_ids  # Add device IDs to the DataFrame\n\n# Plotting\nplt.figure(figsize=(14, 10))\nsns.scatterplot(x='TSNE1', y='TSNE2', hue='PHONEID', palette='tab20', data=tsne_df, legend='full')\nplt.title('t-SNE Visualization of Non-IID data by Device')\nplt.xlabel('t-SNE Axis 1')\nplt.ylabel('t-SNE Axis 1')\nplt.legend(loc='best', bbox_to_anchor=(1.05, 1), title='Device ID')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/UjiIndoorLoc/TrainingData.csv', header=0)\ntest_data = pd.read_csv('/kaggle/input/UjiIndoorLoc/ValidationData.csv', header=0)\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\n\nlabeled_ratio = 0.3\n\nnum_labeled = int(len(train_data) * labeled_ratio)\n\nlabeled_train_data = train_data.iloc[:num_labeled].copy()\nunlabled_train_data = train_data.iloc[num_labeled:].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_data.head())\nprint(test_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform_rss(rss, min_rss=-104, alpha=np.e):\n    if rss < min_rss or rss > 0:\n        return 0\n    else:\n        return ((rss - min_rss)/ -min_rss) ** alpha","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_data(data):\n    features = data.iloc[:, :-9].map(lambda x: transform_rss(x))  # selecting the WAPs\n    labels = data['BUILDINGID'] * 5 + data['FLOOR']  # Extracting the floor and building numbers from dataset\n    return features.to_numpy(), labels.to_numpy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating pytorch datasets\nclass ujiindoor_dataset (Dataset):\n    def __init__ (self, data,labels):\n        if isinstance(data, torch.Tensor):\n            self.data = data.clone().detach().float()\n        else:\n            self.data = torch.tensor(data, dtype=torch.float32)\n        \n        #i am not sure if we want labels to be in tensor form too or just set it with labels\n        if labels is not None:\n            if isinstance(labels, torch.Tensor):\n                self.labels = labels.clone().detach().long()\n            else:\n                self.labels = torch.tensor(labels, dtype=torch.long)\n        else:\n            self.labels = None\n            \n            \n    def __len__ (self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        if self.labels is None:\n            return self.data[index], self.data[index]\n        return self.data[index], self.labels[index]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract features and labels from the data\nlabeled_features,labeled_labels = preprocess_data(labeled_train_data)\nunlabeled_features,unlabeled_labels = preprocess_data(unlabled_train_data)\ntest_features, test_labels = preprocess_data(test_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"autoencoder_labeled_data = ujiindoor_dataset(labeled_features, labeled_labels) #train loader of autoencoder\nlabeled_loader = DataLoader(autoencoder_labeled_data, batch_size=32, shuffle=True)\n\n\nautoencoder_test_data = ujiindoor_dataset(test_features, test_labels) # val loader of autoencoder\ntest_loader = DataLoader(autoencoder_test_data, batch_size=32, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AutoencoderClassifier(nn.Module):\n    def __init__(self, input_dim=520, encoding_dim=64, num_classes=15):\n        super(AutoencoderClassifier, self).__init__()\n        \n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(True),\n            nn.Dropout(0.3),  # Add Dropout\n            nn.Linear(256, 128),\n            nn.ReLU(True),\n            nn.Dropout(0.3),  # Add Dropout\n            nn.Linear(128, encoding_dim),\n            nn.ReLU(True),\n            nn.Dropout(0.3)  # Add Dropout\n        )\n        \n        # Decoder for Autoencoder\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, 128),\n            nn.ReLU(True),\n            nn.Dropout(0.3),  # Add Dropout\n            nn.Linear(128, 256),\n            nn.ReLU(True),\n            nn.Dropout(0.3),  # Add Dropout\n            nn.Linear(256, input_dim),\n            nn.Sigmoid()\n        )\n        \n        # Classifier\n        self.depthwise_conv1 = nn.Conv1d(in_channels=1, out_channels=99, kernel_size=22, groups=1)\n        self.pointwise_conv1 = nn.Conv1d(in_channels=99, out_channels=99, kernel_size=1)\n        self.depthwise_conv2 = nn.Conv1d(in_channels=99, out_channels=66, kernel_size=22, groups=1)\n        self.pointwise_conv2 = nn.Conv1d(in_channels=66, out_channels=66, kernel_size=1)\n        self.depthwise_conv3 = nn.Conv1d(in_channels=66, out_channels=33, kernel_size=22, groups=1)\n        self.pointwise_conv3 = nn.Conv1d(in_channels=33, out_channels=33, kernel_size=1)\n        self.fc1 = nn.Linear(33, 128)\n        self.dropout = nn.Dropout(0.5)  # Add Dropout\n        self.fc2 = nn.Linear(128, num_classes)  # num_classes = number of unique (building, floor) combinations\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        \n        # Decoder part (Autoencoder)\n        decoded = self.decoder(encoded)\n        \n        # Classifier part\n        x = encoded.unsqueeze(1)  # Add channel dimension for Conv1d\n        x = self.depthwise_conv1(x)\n        x = self.pointwise_conv1(x)\n        x = self.depthwise_conv2(x)\n        x = self.pointwise_conv2(x)\n        x = self.depthwise_conv3(x)\n        x = self.pointwise_conv3(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.dropout(x)  # Apply Dropout\n        x = self.fc2(x)\n        \n        return decoded, x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define loss functions\ncriterion_ae = nn.MSELoss()\ncriterion_cls = nn.CrossEntropyLoss()\nweight_decay = 1e-4\n\n# Initialize and train the model\nmodel = AutoencoderClassifier()\noptimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n\n# Training loop\nnum_epochs = 20\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss_ae = 0\n    epoch_loss_cls = 0\n    for inputs, labels in labeled_loader:\n        optimizer.zero_grad()\n        decoded, outputs = model(inputs)\n        loss_ae = criterion_ae(decoded, inputs)\n        loss_cls = criterion_cls(outputs, labels.long()) \n        loss = loss_ae + loss_cls\n        loss.backward()\n        optimizer.step()\n        epoch_loss_ae += loss_ae.item()\n        epoch_loss_cls += loss_cls.item()\n    avg_loss_ae = epoch_loss_ae / len(labeled_loader)\n    avg_loss_cls = epoch_loss_cls / len(labeled_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Autoencoder Loss: {avg_loss_ae:.4f}, Classifier Loss: {avg_loss_cls:.4f}\")\n\n# Save the pre-trained model\ntorch.save(model.state_dict(), 'pretrained_autoencoder_classifier2.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoencoderClassifier()\nmodel.load_state_dict(torch.load('pretrained_autoencoder_classifier2.pth'))\nmodel.eval()  # Set model to evaluation mode\n\n# Define loss functions\ncriterion_ae = nn.MSELoss()\ncriterion_cls = nn.CrossEntropyLoss()\n\n# Create test data loader\n# Assuming test_loader is defined and contains your test dataset\n# Example: test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ntest_loss_ae = 0\ntest_loss_cls = 0\ncorrect_predictions = 0\ntotal_samples = 0\n\nwith torch.no_grad():  # Disable gradient computation for testing\n    for inputs, labels in test_loader:\n        decoded, outputs = model(inputs)\n        \n        # Calculate autoencoder loss\n        loss_ae = criterion_ae(decoded, inputs)\n        test_loss_ae += loss_ae.item()\n        \n        # Calculate classifier loss\n        loss_cls = criterion_cls(outputs, labels.long())  # Convert labels to Long type\n        test_loss_cls += loss_cls.item()\n        \n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct_predictions += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n# Calculate average losses\navg_test_loss_ae = test_loss_ae / len(test_loader)\navg_test_loss_cls = test_loss_cls / len(test_loader)\n\n# Calculate accuracy\naccuracy = correct_predictions / total_samples\n\nprint(f\"Test Autoencoder Loss: {avg_test_loss_ae:.4f}\")\nprint(f\"Test Classifier Loss: {avg_test_loss_cls:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClassifierClient(fl.client.NumPyClient):\n    def __init__(self, cid, net, train_loader, val_loader):\n        self.cid = cid\n        self.net = net.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        \n    def get_parameters(self, config):\n        return [val.cpu().detach().numpy() for val in self.net.parameters()]\n    \n    def set_parameters(self, parameters):\n        for val, param in zip(parameters, self.net.parameters()):\n            param.data = torch.tensor(val, dtype=torch.float32).to(device)\n    \n    def generate_pseudo_labels(self, inputs):\n        self.net.eval()\n        with torch.no_grad():\n            _, outputs = self.net(inputs)\n            _, pseudo_labels = torch.max(outputs, 1)\n        return pseudo_labels\n\n    def fit(self, parameters, config):\n        self.set_parameters(parameters)\n        self.net.train()\n        criterion_ae = nn.MSELoss()\n        criterion_cls = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.net.parameters(), lr=0.0001)  # Reduced learning rate\n        alpha = 1.0  # Scaling factor for classifier loss\n        \n        for epoch in range(3):  # Adjust the number of epochs if needed\n            epoch_loss = 0.0\n            for i, (inputs, _) in enumerate(self.train_loader):\n                inputs = inputs.to(device)\n                \n                # Generate pseudo-labels for the unlabeled data\n                pseudo_labels = self.generate_pseudo_labels(inputs)\n                \n                optimizer.zero_grad()\n                decoded, out = self.net(inputs)\n                \n                # Compute the losses\n                loss_ae = criterion_ae(decoded, inputs)\n                loss_cls = criterion_cls(out, pseudo_labels)\n                loss = loss_ae + alpha * loss_cls\n                \n                # Backpropagation\n                loss.backward()\n                \n                # Gradient clipping\n                torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)\n                \n                optimizer.step()\n                \n                epoch_loss += loss.item()\n                \n                if i % 10 == 9:  # Print every 10 mini-batches\n                    print(f\"[Client {self.cid}, Epoch {epoch + 1}, Batch {i + 1}] loss: {epoch_loss / 10:.4f}\")\n                    epoch_loss = 0.0\n\n            print(f\"Client {self.cid}, Epoch {epoch + 1}, Loss: {epoch_loss / len(self.train_loader)}\")\n            \n        return self.get_parameters(config), len(self.train_loader.dataset), {}\n    \n    def evaluate(self, parameters, config):\n        self.set_parameters(parameters)\n        self.net.eval()\n        criterion_cls = nn.CrossEntropyLoss()\n        \n        loss_cls = 0.0\n        correct_cls = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, labels in self.val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                _, out = self.net(inputs)\n                \n                loss_cls += criterion_cls(out, labels).item()\n                \n                _, predicted_cls = torch.max(out.data, 1)\n                \n                total += labels.size(0)\n                correct_cls += (predicted_cls == labels).sum().item()\n        \n        loss = loss_cls / total\n        accuracy_cls = 100 * correct_cls / total\n        \n        return loss, len(self.val_loader.dataset), {\"accuracy\": accuracy_cls}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clients = []\nnum_clients = 10\nunlabeled_client_datasets = torch.chunk(torch.tensor(unlabeled_features, dtype=torch.float32),num_clients)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pretrained_weights = torch.load('pretrained_autoencoder_classifier2.pth')\n\nprint(f\"Number of chunks: {len(unlabeled_client_datasets)}\")    \nfor cid, client_datasets in enumerate(unlabeled_client_datasets):\n    print(f\"Chunk {cid} size: {client_datasets.size(0)}\")\n    client_net = AutoencoderClassifier()\n    client_net.load_state_dict(pretrained_weights)\n    client_data = DataLoader(ujiindoor_dataset(client_datasets, None), batch_size=32, shuffle=True)\n    print(f\"Training DataLoader for client {cid} has {len(client_data.dataset)} samples\")\n    print(client_data.dataset)\n\n    test_loader = DataLoader(autoencoder_test_data, batch_size=32, shuffle=False)\n  \n    client = ClassifierClient(cid, client_net, client_data, test_loader) #during evaluation this might cause issues\n    clients.append(client)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimilarityAggregationStrategy(FedAvg):\n    def __init__(self, initial_rounds: int = 2, gamma: float = 0.5, similarity_threshold: float = 0.5, num_similar_clients: int = 4):\n        super().__init__()\n        self.initial_rounds = initial_rounds\n        self.gamma = gamma\n        self.similarity_threshold = similarity_threshold\n        self.num_similar_clients = num_similar_clients\n        self.client_histories = {}\n        self.global_round = 0\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: List[Tuple[ClientProxy, FitRes]],\n        failures: List[BaseException],\n    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n        \n        # Convert Parameters to list of NumPy arrays\n        weights = [parameters_to_ndarrays(r[1].parameters) for r in results]\n        if self.global_round < self.initial_rounds:\n            aggregated_weights = self.fedavg(weights)\n        else:\n            aggregated_weights = self.similarity_aggregate(weights, results)\n        \n        self.global_round += 1\n\n        # Save the final aggregated weights\n        if self.global_round == 5:\n            print(\"Saving final aggregated weights...\")\n            self.save_model(aggregated_weights, \"/kaggle/working/final_aggregated_model.pth\")\n\n        return ndarrays_to_parameters(aggregated_weights), {}\n\n    def fedavg(self, weights: List[NDArrays]) -> List[np.ndarray]:\n        avg_weights = [np.mean([w[i] for w in weights], axis=0) for i in range(len(weights[0]))]\n        return avg_weights\n\n    def similarity_aggregate(self, weights: List[NDArrays], results: List[Tuple[ClientProxy, FitRes]]) -> List[np.ndarray]:\n        similarities = self.calculate_similarities(weights, results)\n        aggregated_weights = []\n        for i, (client, fit_res) in enumerate(results):\n            similar_clients = [j for j, sim in enumerate(similarities[i]) if sim > self.similarity_threshold]\n            if len(similar_clients) > self.num_similar_clients:\n                similar_clients = np.argsort(similarities[i])[-self.num_similar_clients:]\n            similar_weights = [weights[j] for j in similar_clients]\n            client_weight = weights[i]\n            aggregated_weight = np.mean([client_weight] + similar_weights, axis=0)\n            aggregated_weights.append(aggregated_weight)\n        final_aggregated_weights = [np.mean([w[i] for w in aggregated_weights], axis=0) for i in range(len(weights[0]))]\n        return final_aggregated_weights\n\n    def calculate_similarities(self, weights: List[NDArrays], results: List[Tuple[ClientProxy, FitRes]]) -> List[List[float]]:\n\n        similarities = []\n        for i in range(len(results)):\n            client_similarities = []\n            for j in range(len(results)):\n                if i == j:\n                    client_similarities.append(1.0)\n                else:\n                    sim = self.calculate_similarity(weights[i], weights[j])\n                    client_similarities.append(sim)\n            similarities.append(client_similarities)\n        return similarities\n\n    def calculate_similarity(self, weight_i: NDArrays, weight_j: NDArrays) -> float:\n        grad_i = [wi - w0 for wi, w0 in zip(weight_i, self.client_histories.get(i, weight_i))]\n        grad_j = [wj - w0 for wj, w0 in zip(weight_j, self.client_histories.get(j, weight_j))]\n        acc_grad_i = [wi - w0 for wi, w0 in zip(weight_i, self.client_histories.get(i, weight_i))]\n        acc_grad_j = [wj - w0 for wj, w0 in zip(weight_j, self.client_histories.get(j, weight_j))]\n\n        similarity_term_1 = np.dot(grad_i, grad_j) / (np.linalg.norm(grad_i) * np.linalg.norm(grad_j))\n        similarity_term_2 = np.dot(acc_grad_i, acc_grad_j) / (np.linalg.norm(acc_grad_i) * np.linalg.norm(acc_grad_j))\n        \n        similarity = self.gamma * similarity_term_1 + (1 - self.gamma) * similarity_term_2\n        return similarity\n\n    def save_model(self, weights: List[np.ndarray], filename: str):\n        # Create a dummy model to generate the correct state_dict\n        dummy_model = AutoencoderClassifier()\n        state_dict = dummy_model.state_dict()\n        \n        # Map the list of weights to the state_dict\n        for (key, param), weight in zip(state_dict.items(), weights):\n            state_dict[key] = torch.tensor(weight)\n        \n        # Save the state_dict\n        torch.save(state_dict, filename)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def client_fn(cid) -> fl.client.Client:\n    return clients[int(cid)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strategy = SimilarityAggregationStrategy()\n\nfl.simulation.start_simulation(\n    client_fn=client_fn,\n    num_clients=num_clients,\n    strategy=strategy,\n    config=ServerConfig(num_rounds=5),\n    client_resources={'num_gpus': 1, 'num_cpus': 1},\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global_model = AutoencoderClassifier()\nglobal_model.load_state_dict(torch.load('/kaggle/input/fd/other/default/1/final_aggregated_model.pth'))\nglobal_model.eval()  # Set model to evaluation mode\n\n# Define loss functions\ncriterion_ae = nn.MSELoss()\ncriterion_cls = nn.CrossEntropyLoss()\n\n\ntest_loss_ae = 0\ntest_loss_cls = 0\ncorrect_predictions = 0\ntotal_samples = 0\n\nwith torch.no_grad():  # Disable gradient computation for testing\n    for inputs, labels in test_loader:\n        decoded, outputs = global_model(inputs)\n        \n        # Calculate autoencoder loss\n        loss_ae = criterion_ae(decoded, inputs)\n        test_loss_ae += loss_ae.item()\n        \n        # Calculate classifier loss\n        loss_cls = criterion_cls(outputs, labels.long())  # Convert labels to Long type\n        test_loss_cls += loss_cls.item()\n        \n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct_predictions += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n# Calculate average losses\navg_test_loss_ae = test_loss_ae / len(test_loader)\navg_test_loss_cls = test_loss_cls / len(test_loader)\n\n# Calculate accuracy\nfederated_accuracy = correct_predictions / total_samples\n\nprint(f\"Test Autoencoder Loss: {avg_test_loss_ae:.4f}\")\nprint(f\"Test Classifier Loss: {avg_test_loss_cls:.4f}\")\nprint(f\"Test Accuracy: {federated_accuracy:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"testy = DataLoader(autoencoder_test_data, batch_size=32, shuffle=False)\n\n# Running the model to get predictions\nall_predictions = []\nall_labels = []\n\nwith torch.no_grad():\n    for inputs, labels in testy:\n        inputs = inputs\n        labels = labels\n        \n        _, outputs = global_model(inputs)\n        \n        _, predicted = torch.max(outputs, 1)\n        \n        all_predictions.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Compare predictions with the true labels\nfor i in range(len(all_predictions)):\n    print(f\"Predicted: {all_predictions[i]}, True Label: {all_labels[i]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"center = AutoencoderClassifier()\ncriterion_ae = nn.MSELoss()\noptimizer = optim.Adam(center.parameters(), lr=0.001, weight_decay=1e-4)\n\n# Create DataLoader for unlabeled data\nunlabeled_dataset = ujiindoor_dataset(unlabeled_features, None)\nunlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n\n# Training loop for the autoencoder part only\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    center.train()\n    epoch_loss_ae = 0\n    for inputs, _ in unlabeled_loader:\n        optimizer.zero_grad()\n        decoded, _ = center(inputs)\n        loss_ae = criterion_ae(decoded, inputs)\n        loss_ae.backward()\n        optimizer.step()\n        epoch_loss_ae += loss_ae.item()\n    avg_loss_ae = epoch_loss_ae / len(unlabeled_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Autoencoder Loss: {avg_loss_ae:.4f}\")\n\n# Save the autoencoder model\ntorch.save(center.state_dict(), 'autoencoder_classifier_unlabeled2.pth')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the trained autoencoder model\ncenter.load_state_dict(torch.load('autoencoder_classifier_unlabeled2.pth'))\ncenter.eval()\n\n# Generate pseudo-labels using the trained autoencoder's encoder part\npseudo_labels = []\nwith torch.no_grad():\n    for inputs, _ in unlabeled_loader:\n        _, outputs = center(inputs)\n        _, predicted = torch.max(outputs, 1)\n        pseudo_labels.extend(predicted.cpu().numpy())\n\npseudo_labels = np.array(pseudo_labels)\n\n# Create a new dataset with pseudo-labels\npseudo_labeled_dataset = ujiindoor_dataset(unlabeled_features, pseudo_labels)\npseudo_labeled_loader = DataLoader(pseudo_labeled_dataset, batch_size=32, shuffle=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine labeled and pseudo-labeled data\ncombined_loader = DataLoader(\n    ujiindoor_dataset(np.concatenate([labeled_features, unlabeled_features]), \n                     np.concatenate([labeled_labels, pseudo_labels])),\n    batch_size=32, shuffle=True\n)\n\n# Reinitialize the optimizer for the combined training\noptimizer = optim.Adam(center.parameters(), lr=0.001, weight_decay=1e-4)\n\n# Training loop for both labeled and pseudo-labeled data\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    center.train()\n    epoch_loss_ae = 0\n    epoch_loss_cls = 0\n    for inputs, labels in combined_loader:\n        optimizer.zero_grad()\n        decoded, outputs = center(inputs)\n        loss_ae = criterion_ae(decoded, inputs)\n        loss_cls = criterion_cls(outputs, labels.long())\n        loss = loss_ae + loss_cls\n        loss.backward()\n        optimizer.step()\n        epoch_loss_ae += loss_ae.item()\n        epoch_loss_cls += loss_cls.item()\n    avg_loss_ae = epoch_loss_ae / len(combined_loader)\n    avg_loss_cls = epoch_loss_cls / len(combined_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Autoencoder Loss: {avg_loss_ae:.4f}, Classifier Loss: {avg_loss_cls:.4f}\")\n\n# Save the combined trained model\ntorch.save(model.state_dict(), 'autoencoder_classifier_combined3.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the combined trained model\nzz = AutoencoderClassifier()\nzz.load_state_dict(torch.load('autoencoder_classifier_combined3.pth'))\nzz.eval()  # Set model to evaluation mode\n\n# Create DataLoader for test data\ntest_dataset = ujiindoor_dataset(test_features, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Evaluation loop for the autoencoder and classifier\ntest_loss_ae = 0\ntest_loss_cls = 0\ncorrect_predictions = 0\ntotal_samples = 0\n\nwith torch.no_grad():  # Disable gradient computation for testing\n    for inputs, labels in test_loader:\n        decoded, outputs = zz(inputs)\n        \n        # Calculate autoencoder loss\n        loss_ae = criterion_ae(decoded, inputs)\n        test_loss_ae += loss_ae.item()\n        \n        # Calculate classifier loss\n        loss_cls = criterion_cls(outputs, labels.long())\n        test_loss_cls += loss_cls.item()\n        \n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        correct_predictions += (predicted == labels).sum().item()\n        total_samples += labels.size(0)\n\n# Calculate average losses\navg_test_loss_ae = test_loss_ae / len(test_loader)\navg_test_loss_cls = test_loss_cls / len(test_loader)\n\n# Calculate accuracy\ncentralized_accuracy = correct_predictions / total_samples\n\nprint(f\"Test Autoencoder Loss: {avg_test_loss_ae:.4f}\")\nprint(f\"Test Classifier Loss: {avg_test_loss_cls:.4f}\")\nprint(f\"Test Accuracy: {centralized_accuracy:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracies\naccuracies = [federated_accuracy, centralized_accuracy]\nlabels = ['Federated Learning with Similarity Strategy', 'Centralized Learning']\n\n# Plotting the accuracies\nplt.figure(figsize=(10, 6))\n\n# Bar plot\nplt.bar(labels, accuracies, color=['blue', 'green'])\n\nplt.xlabel('Learning Approach (100 Epochs)')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Comparison: Federated vs Centralized Learning (NON-IID)')\nplt.ylim(0, 1)  # Assuming accuracy is between 0 and 1\nplt.grid(True)\n\n# Adding the accuracy values on top of the bars\nfor i, v in enumerate(accuracies):\n    plt.text(i, v + 0.02, f\"{v:.2%}\", ha='center', fontweight='bold')\n\nplt.savefig('accuracy_comparison.png')  \nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}